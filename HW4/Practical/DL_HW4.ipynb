{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_HW4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgIvhkOvLBTK"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;\">\n",
        "\t<font face=\"XB Zar\" size=5>\n",
        "\t\t<div align=center>\n",
        "\t\t\t<font face=\"IranNastaliq\" size=30>\n",
        "\t\t\t\t<p></p>\n",
        "\t\t\t\t<p></p>\n",
        "به نام خدا\n",
        "\t\t\t\t<p></p>\n",
        "\t\t\t</font>\n",
        "\t\t\t<font color=#FF7500>\n",
        "دانشگاه صنعتی شریف - دانشکده مهندسی کامپیوتر\n",
        "            </font>\n",
        "\t\t\t<p></p>\n",
        "\t\t\t<font color=blue>\n",
        "یادگیری ژرف\n",
        "            </font>\n",
        "\t\t\t<br />\n",
        "\t\t\t<br />\n",
        "پاییز ۱۴۰۰\t\t</div>\n",
        "\t\t<hr/>\n",
        "\t\t<div align=center>\n",
        "\t\t    <font size=6>\n",
        "\t\t\t    <br />\n",
        "تمرین چهارم - Autoencoders - Attention Models\n",
        "            \t<br/>\n",
        "\t\t\t</font>\n",
        "طراح: افشین کریمی\n",
        "                <br/><br/>\n",
        "                <br/>\n",
        "        </style>\n",
        "\t</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fedupasRLBQa"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;\">\n",
        "\t<font face=\"XB Zar\" size=5>\n",
        "\t\t<div align=center>\n",
        "\t\t\t\t<p></p>\n",
        "\t\t\t\t<p></p>\n",
        "                نام:\n",
        "\t\t\t\t<br/>\n",
        "                شماره دانشجویی:\n",
        "        </div>\n",
        "\t\t<br />\n",
        "\t\t<hr />\n",
        "\t\t<style type=\"text/css\" scoped>\n",
        "        p{\n",
        "        border: 1px solid #a2a9b1;background-color: #f8f9fa;display: inline-block;\n",
        "        };\n",
        "        </style>\n",
        "\t</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XklhkNc0REPo"
      },
      "source": [
        "# Autoencoders\n",
        "\n",
        "Just Complete the ToDo Parts "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07jYTmTtVqMn"
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIqrs9fAVqJ5"
      },
      "source": [
        "# Transforms images to a PyTorch Tensor\n",
        "tensor_transform = transforms.ToTensor()\n",
        "  \n",
        "# Download the MNIST Dataset\n",
        "dataset = datasets.MNIST(root = \"./data\",\n",
        "                         train = True,\n",
        "                         download = True,\n",
        "                         transform = tensor_transform)\n",
        "  \n",
        "# DataLoader is used to load the dataset \n",
        "# for training\n",
        "loader = torch.utils.data.DataLoader(dataset = dataset,\n",
        "                                     batch_size = 32,\n",
        "                                     shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYXxhKMkVqHH"
      },
      "source": [
        "# Creating a PyTorch class\n",
        "# 28*28 ==> 9 ==> 28*28\n",
        "class AE(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "          \n",
        "        ''' Todo: Build a linear encoder with Linear\n",
        "         layer followed by Relu activation function\n",
        "         784 ==> 9 '''\n",
        "        self.encoder = None\n",
        "          \n",
        "        ''' Todo: Build a linear decoder with Linear\n",
        "         layer followed by Relu activation function\n",
        "         The Sigmoid activation function\n",
        "         outputs the value between 0 and 1\n",
        "         9 ==> 784 '''\n",
        "        self.decoder = None\n",
        "  \n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfals6CUVqCW"
      },
      "source": [
        "\n",
        "''' Todo: Initialaize model '''\n",
        "model = None\n",
        "  \n",
        "''' Todo: Validation using MSE Loss function '''\n",
        "loss_function = None\n",
        "  \n",
        "''' Todo: Use an Adam Optimizer with lr = 0.1 '''\n",
        "optimizer = None"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-3OCC8tVp_n"
      },
      "source": [
        "epochs = 20\n",
        "outputs = []\n",
        "losses = []\n",
        "for epoch in range(epochs):\n",
        "    for (image, _) in loader:\n",
        "        \n",
        "      ''' Todo: Reshaping the image to (-1, 784) '''\n",
        "      image = None\n",
        "        \n",
        "      # Output of Autoencoder\n",
        "      reconstructed = model(image)\n",
        "        \n",
        "      ''' Todo: Calculate the loss function '''\n",
        "      loss = None\n",
        "        \n",
        "      # The gradients are set to zero,\n",
        "      # the the gradient is computed and stored.\n",
        "      # .step() performs parameter update\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "        \n",
        "      # Storing the losses in a list for plotting\n",
        "      losses.append(loss)\n",
        "    outputs.append((epochs, image, reconstructed))\n",
        "  \n",
        "# Defining the Plot Style\n",
        "plt.style.use('fivethirtyeight')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "  \n",
        "''' Todo: Plot the last 100 values '''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtwSxSiCbkoR"
      },
      "source": [
        "# Plot the first input image array\n",
        "for i, item in enumerate(image):\n",
        "    \n",
        "  # Reshape the array for plotting\n",
        "  item = item.reshape(-1, 28, 28)\n",
        "  plt.imshow(item[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVHE0U6aegso"
      },
      "source": [
        "''' Todo: Plot the first reconstructed input image array '''  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbk_vMckkhCp"
      },
      "source": [
        "# Attention Models\n",
        "\n",
        "Just Complete the ToDo Parts "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJixqDSwK4J8"
      },
      "source": [
        "# Some imports that we require to write the network.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzscV_sXkuYJ"
      },
      "source": [
        "# Encoder for the attention network that is similar to the vanilla encoders\n",
        "class Encoder(nn.Module):\n",
        "  # Store the parameters\n",
        "  def __init__(self, input_size, hidden_size, bidirectional = True):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.input_size = input_size\n",
        "    self.bidirectional = bidirectional\n",
        "    \n",
        "    ''' ToDo : Create an LSTM layer '''\n",
        "    self.lstm = None\n",
        "  \n",
        "  # The Forward function\n",
        "  def forward(self, inputs, hidden):\n",
        "    \n",
        "    ''' Todo : Pass the input through the LSTM with the provided hidden state '''\n",
        "    output, hidden = None\n",
        "    return output, hidden\n",
        "    \n",
        "  # This function has to be called before passing sentence through the LSTM to initialize the hidden state.  \n",
        "  def init_hidden(self):\n",
        "    return (torch.zeros(1 + int(self.bidirectional), 1, self.hidden_size),\n",
        "      torch.zeros(1 + int(self.bidirectional), 1, self.hidden_size))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S92J-YHJmvb3"
      },
      "source": [
        "# This class is the attention based decoder\n",
        "class AttentionDecoder(nn.Module):\n",
        "  \n",
        "  def __init__(self, hidden_size, output_size, vocab_size):\n",
        "    super(AttentionDecoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    \n",
        "    # This layer calculates the importance of the word, by using the previous decoder hidden state and the hidden state of the encoder at that particular time step\n",
        "    self.attn = nn.Linear(hidden_size + output_size, 1)\n",
        "    ''' Todo: The 'lstm' layer takes in concatenation of vector obtained by having a weighted sum according to attention weights and the previous word outputted '''\n",
        "    self.lstm = None\n",
        "    ''' Todo: Map the output feature space into the size of vocabulary '''\n",
        "    self.final = None\n",
        "  \n",
        "  # The 'init_hidden' function is used in the same way as in the encoder.\n",
        "  def init_hidden(self):\n",
        "    return (torch.zeros(1, 1, self.output_size),\n",
        "      torch.zeros(1, 1, self.output_size))\n",
        "  \n",
        "  # The forward function of the decoder\n",
        "  def forward(self, decoder_hidden, encoder_outputs, input):\n",
        "    \n",
        "    # 'weights' list is used to store the attention weights\n",
        "    weights = []\n",
        "    for i in range(len(encoder_outputs)):\n",
        "      print(decoder_hidden[0][0].shape)\n",
        "      print(encoder_outputs[0].shape)\n",
        "      # Pass each encoder output through the 'attn' layer along with \n",
        "      # decoder's previous hidden state by concatenating them and store \n",
        "      # them in the 'weights' list \n",
        "      weights.append(self.attn(torch.cat((decoder_hidden[0][0], \n",
        "                                          encoder_outputs[i]), dim = 1)))\n",
        "      \n",
        "    ''' Todo : scale weights in range (0,1) by applying softmax activation '''\n",
        "    normalized_weights = None\n",
        "    \n",
        "    # To calculate the weighted sum, we use batch matrix multiplication\n",
        "    attn_applied = torch.bmm(normalized_weights.unsqueeze(1),\n",
        "                             encoder_outputs.view(1, -1, self.hidden_size))\n",
        "    \n",
        "    input_lstm = torch.cat((attn_applied[0], input[0]), dim = 1) #if we are using embedding, use embedding of input here instead\n",
        "    \n",
        "    output, hidden = self.lstm(input_lstm.unsqueeze(0), decoder_hidden)\n",
        "    \n",
        "    output = self.final(output[0])\n",
        "    \n",
        "    return output, hidden, normalized_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqIG8wCFvqO3"
      },
      "source": [
        "# Testing the code\n",
        "bidirectional = True\n",
        "c = Encoder(10, 20, bidirectional)\n",
        "a, b = c.forward(torch.randn(10), c.init_hidden())\n",
        "print(a.shape)\n",
        "print(b[0].shape)\n",
        "print(b[1].shape)\n",
        "\n",
        "x = AttentionDecoder(20 * (1 + bidirectional), 25, 30)\n",
        "y, z, w = x.forward(x.init_hidden(), torch.cat((a,a)), torch.zeros(1,1, 30)) \n",
        "print(y.shape)\n",
        "print(z[0].shape)\n",
        "print(z[1].shape)\n",
        "print(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_hUjaFHvv8C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}